# -*- coding: utf-8 -*-
"""SARSA(lambda,Backward view).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_N0sGF3O8CdoyzrMrywvv1-tUd7J1aP8
"""

!pip install gym-minigrid
import gym_minigrid
import numpy as np
import gym
import matplotlib.pyplot as plt
import random


#environment generation
env=gym.make('MiniGrid-Empty-6x6-v0')#render_mode="human")
env.reset()


epsilon=1
alpha=.2
gamma=.7
epsiodes=400
lambdas=.5
time_step=[]
reward_episode=[]


#initializing q_values
q_values={}
for x in range(1,5):
  for y in range(1,5):
    for z in range(4):
      q_values[((x,y),z)]={0:0,1:0,2:0}


'''eligibility trace tells how many times or how frequent that state-action pair is coming.It consists of frequency huristic means deter
mining state-action pair on the frequency of previous state-action pair,It also consists of recency huristic means determining state-action
pair based on the history of recent state-action pair.The state-action pair having more eligibility trace will be updated more as compared
to state-action pair having low eligibility trace.'''
eligibility_trace={}
for x in range(1,5):
  for y in range(1,5):
    for z in range(4):
      for w in range(3):
        eligibility_trace[((x,y),z),w]=0


'''here how to choose a action is written basically If we randomly choose actions without using q_value then the learning is not nice
    so I have used chatgpt to know how can I use q_value or involve the q_value in taking actions...'''
def action_generator(epsilon,q):
  if(random.random()<epsilon):
    act=random.randint(0,2)
  else:
    act=max(q, key=q.get)
  return act


'''This is function which will decay epsilon,Reason for this is that if we do (epsilon/no. of episodes) only 
 then it may happens that due to some reason epsilon becomes too small which will affect our exploration so a minimum value is required.''' 
def epsilon_decay(old_epsilon,decay_rate,min_epsilon):
  epi=old_epsilon*decay_rate
  return max(min_epsilon,epi)


def qvalues_generator(env,q_values,epsilon,alpha,gamma,lambdas,eligibility_trace):
  env.reset()
  state=(env.agent_pos,env.agent_dir)
  '''terminal state q_values=0'''
  q_values[(4,4),]=0
  t=0
  G=0
  while(state[0]!=(4,4)):
    action=action_generator(epsilon,q_values[state])

    '''So basically at each time step of episode we update the eligibility trace of each state-action pair,we decay eligibility
    trace over time,but present state,action pair eligibility trace is increased by one'''
    for i in eligibility_trace.keys():
      if(i==(state,action)):
        eligibility_trace[i]=eligibility_trace[i]+1
      else:
        eligibility_trace[i]=gamma*lambdas*eligibility_trace[i]

    obs,reward,done,truncated,info=env.step(action)
    G=reward+(gamma)**(t)*(G)
    next_state=(env.agent_pos,env.agent_dir)
    next_action=action_generator(epsilon,q_values[next_state])

    '''now calculation of TD-Error is there '''
    td_error=reward+gamma*(q_values[next_state][next_action])-q_values[state][action]
    
    '''This is one of the big difference between sarsa backward view and other algorithm. In other algorithm each state-action pair is updated
    only once in a episode until it occurs again, But here case is different at each time step we are updating each state action pair q_value'''
    for i in eligibility_trace.keys():
      state1=i[0]
      action1=i[1]
      q_values[state1][action1]=q_values[state1][action1]+alpha*td_error*eligibility_trace[i]
    state=next_state
    t=t+1
  return q_values,t,G


'''Here I have applied a loop,,q_value generator.....inside that--->(action_generator-->epsilon decay--->updating Eligibility Trace-->
calculating Tderror)'''
episode_no=0
while(episode_no<=epsiodes):
  q_values,t,G=qvalues_generator(env,q_values,epsilon,alpha,gamma,lambdas,eligibility_trace)
  time_step.append(t)
  reward_episode.append(G)
  epsilon=epsilon_decay(epsilon,decay_rate=.95,min_epsilon=0.01)
  episode_no=episode_no+1


print(q_values)
print(time_step)
print(reward_episode)
episod=np.arange(1,len(time_step)+1)
plt.plot(episod,time_step)
plt.xlabel('Episodes')
plt.ylabel('steps')

episod=np.arange(1,len(reward_episode)+1)
plt.plot(episod,reward_episode)
plt.xlabel('Episodes')
plt.ylabel('reward')