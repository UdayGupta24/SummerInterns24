# -*- coding: utf-8 -*-
"""25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HMUSF3NBKkDCMmBJBCB8qSIxiqeAjSoD
"""

!pip install gym-minigrid
import gym_minigrid
import numpy as np
import gym
import matplotlib.pyplot as plt
import random


#environment generation
env=gym.make('MiniGrid-Empty-6x6-v0')#render_mode="human")
env.reset()


#initializing q_values
q_values={}
for x in range(1,5):
  for y in range(1,5):
    for z in range(4):
      q_values[((x,y),z)]={0:0,1:0,2:0}


epsilon=1
alpha=.2
gamma=.7
epsiodes=400
time_step=[]
reward_episode=[]


def qvalues_generator(env,q_values,epsilon,alpha,gamma):
  env.reset()
  state=(env.agent_pos,env.agent_dir)
  '''terminal state q_values=0'''
  q_values[(4,4),]=0
  t=0
  G=0
  while(state[0]!=(4,4)):
    action=action_generator(epsilon,q_values[state])
    _,reward,done,_,_=env.step(action)
    G=reward+(gamma)**(t)*(G)
    next_state=(env.agent_pos,env.agent_dir)

    '''In order to update q_value of present state we are required to have next state maximum q_value'''
    max_action=max(q_values[next_state],key=q_values[next_state].get)
    max_of_q=q_values[next_state][max_action]


    '''updating q_value of present state'''
    q_values[state][action]=q_values[state][action]+alpha*(reward+gamma*(max_of_q) - q_values[state][action])
    state=next_state
    t=t+1
  return q_values,t,G

'''here how to choose a action is written basically If we randomly choose actions without using q_value then the learning is not nice
    so I have used chatgpt to know how can I use q_value or involve the q_value in taking actions...'''
def action_generator(epsilon,q):
  if(random.random()<epsilon):
    act=random.randint(0,2)
  else:
    act=max(q, key=q.get)
  return act


'''This is function which will decay epsilon,Reason for this is that if we do (epsilon/no. of episodes) only 
 then it may happens that due to some reason epsilon becomes too small which will affect our exploration so a minimum value is required.''' 
def epsilon_decay(old_epsilon,decay_rate=.99,min_epsilon=0.01):
  epi=old_epsilon*decay_rate
  return max(min_epsilon,epi)


episode_no=0
'''here I applied a loop ,,q_values_generator---->inside that--->action_generator--->inside that--->epsilon decay'''
while(episode_no<=epsiodes):
  q_values,t,G=qvalues_generator(env,q_values,epsilon,alpha,gamma)
  time_step.append(t)
  reward_episode.append(G)
  epsilon=epsilon_decay(epsilon,decay_rate=.99,min_epsilon=0.01)
  episode_no=episode_no+1


print(q_values)
print(time_step)
print(reward_episode)


episod=np.arange(1,len(time_step)+1)
plt.plot(episod,time_step)
plt.xlabel('Episodes')
plt.ylabel('steps')
episod=np.arange(1,len(reward_episode)+1)
plt.plot(episod,reward_episode)
plt.xlabel('Episodes')
plt.ylabel('reward')